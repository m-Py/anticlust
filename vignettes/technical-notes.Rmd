---
title: "Technical notes on the `anticlust` package"
author: "Martin Papenberg"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_book:
    toc: false
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

bibliography: lit.bib
csl: "apa.csl"

header-includes:
  - \usepackage{hyperref}
  - \hypersetup{colorlinks = true}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.cap = "",
  message = FALSE,
  warning = FALSE
)

set.seed(123)
```

This document explains the technical and algorithmical background of the
`R` package `anticlust`. **It is still a work in progress.** The
following topics are covered:

1. A formalization of the anticlustering problem
2. A description of the objective functions used to measure anticluster
   similarity
3. A documentation of the algorithms used to optimize the objective
   funtions

# Problem formalization

A set of $n$ d-dimensional data points $X = \{x_i\}$ ($i \in \{1, ...,
n\}$) has to be partitioned into $K$ clusters $C = \{c_k, k = 1, ...,
K\}$, satisfying the following restrictions:

\begin{align}
\bigcup\limits_{k = 1}^{K} c_k = X \label{formalization:1} \\
S_k \cap S_j = \emptyset, \; \forall k, j \in \{1, ..., K\}, \; k \ne j\label{formalization:2} \\
\vert c_k \vert = \vert c_j \vert, \; \forall k, j \in \{1, ..., K\} \label{formalization:3}
\end{align}

Restriction (1) ensures that each element from the underlying set $X$ is
assigned to an anticluster; restriction (2) ensures that each element is
assigned to only one anticluster; restriction (3) ensures that each
anticluster contains the same number of elements. It follows that $\vert
c_k \vert = \frac{n}{K}$ $\forall k \in \{1, ..., K\}$. Restriction (3)
is currently implemented for all methods in the `anticlust` package, but
it is not an obligatory restriction for anticlustering in general.The
objective is to select a partitioning that maximizes the similarity of
the $K$ anticlusters.


# Objective functions

This section presents definitions of optimal anticluster similarity as
assumed in the `anticlust` package.

## The variance objective

@spath1986 and @valev1998 independently proposed to maximize the
variance criterion used in k-means clustering as the objective in
anticlustering. The variance criterion is given by sum of the squared
errors between cluster centers ($\mu_k$) and individual data points
[@jain2010]:

\begin{align} \label{varianceobj}
\sum\limits_{k=1}^{K} \sum\limits_{x_i \in c_k} \vert \vert x_i -
\mu_k \vert \vert^2
\end{align}

The following plot graphically illustrates efforts to maximize and to
minimize the variance criterion in a 2-dimensional feature space for
three (anti)clusters, respectively. The partitions employ restrictions
\eqref{formalization:1} - \eqref{formalization:3}, including the
restriction of equal (anti)cluster sizes. Optimizing the variance
objective is a computationally difficult problem that is usually tackled
using heuristic methods [@jain2010; @spath1986; @valev1998].

```{r, fig.width = 6.5, fig.height = 3.7, echo = FALSE, fig.cap = "Attempts to minimize and maximize the variance objective, leading to clustering and anticlustering partitions, respectively. **TODO**: add cluster centers to illustrate the k-means variance objective; in anticlustering, the cluster centers are close to each other."}
library("anticlust")
n_elements <- 90
features <- matrix(runif(n_elements * 2), ncol = 2)
n_groups <- 3
pch <- c(15, 17, 19)
col <- c("gray8", "gray30", "gray78")

clusters <- clustering(features, n_groups)
anticlusters <- anticlustering(features, n_groups,
                               method = "sampling",
                               objective = "variance",
                               nrep = 1000)

# Plot clusters and anticlusters
par(mfrow = c(1, 2))
plot_clusters(features, clusters, pch = pch,
              main = "Minimize variance", col = col)
plot_clusters(features, anticlusters, pch = pch,
              main = "Maximize variance", col = col)
```

## The distance objective

In addition to the variance criterion, the `anticlust` package
introduces another clustering objective to the anticlustering
application. The objective has been developed in the problem domain of
cluster editing and is based on a measure of the pairwise dissimilarity
between data points [@rahmann2007; @bocker2013].[^1] In weighted cluster
editing, the optimal objective is found when the sum of all pairwise
dissimilarities within-clusters is minimized; for the anticlustering
application, the sum of pairwise dissimilarities is maximized instead.

To formalize the cluster editing objective, we use variables $x_{ij}$ to
encode whether two data points $x_i$ and $x_j$ belong to the same
anticluster $c_k$:

[^1]: Cluster editing has also been studied under different names such
as correlation clustering [@bansal2004], clique partition problem
[@grotschel1989], and transitivity clustering [@wittkop2010].

\begin{align} \label{decisionvars}
   x_{ij} =
   \begin{cases}
     1 & \textrm{if} \:  \: x_i \in c_k \wedge x_j \in c_k \\
     0 & \textrm{otherwise}
   \end{cases}
\end{align}

Assume that $d_{ij}$ represents a measure of the dissimilarity between
two data points $x_i$ and $x_j$, for example given as the euclidean
distance. The cluster editing distance objective is then given as
follows [@miyauchi2015; @grotschel1989]:

\begin{align} \label{objectivedist}
\sum\limits_{1 \leq i < j \leq n} d_{ij} \: x_{ij}
\end{align}

I refer to this objective function as the "distance objective" as
opposed to the "variance objective" in \eqref{varianceobj}.

Maximizing the distance objective corresponds to minimizing the average
linkage distance between anticlusters. In hierarchical cluster
algorithms, the average linkage distance is a quantification of the
similarity of two clusters [@bacon2001]. To appreciate the
correspondence of the distance objective and the average linkage method,
consider the total sum of all pairwise dissimilarities. The total sum
can be partitioned into the sum of distances within-clusters and the sum
of distances between-clusters:

\begin{align}
\sum\limits_{1 \leq i < j \leq n} d_{ij} =
\sum\limits_{1 \leq i < j \leq n} d_{ij} \: x_{ij} +
\sum\limits_{1 \leq i < j \leq n} d_{ij} \: (1- x_{ij})
\end{align}

The total sum of distances is not influenced by the concrete
partitioning $x_{ij}$; hence, the following optimizations lead to the
same partitions, i.e., assignments of elements to anticlusters:

$$\mathrm{Maximize} \sum\limits_{1 \leq i < j \leq n} d_{ij} \: x_{ij}$$

$$\mathrm{Minimize} \sum\limits_{1 \leq i < j \leq n} d_{ij} \: (1- x_{ij})$$

In the special case of $K = 2$ where we have two partitions $A$ and $B$,
the sum of the between-cluster distances can be formulated as follows:

\begin{align} \label{sumlinkage}
\sum\limits_{i \in A} \: \sum\limits_{j \in B} d_{ij}
\end{align}

This formulation is very close to the average linkage objective that
additioanlly incorporates the cardinalities of the sets $A$ and $B$
[@guha1998]:

\begin{align} \label{avglinkage}
\frac{1}{\vert A \vert \: \vert B \vert }
\sum\limits_{i \in A} \: \sum\limits_{j \in B} d_{ij}
\end{align}

Given restriction \eqref{formalization:3} for the anticlustering
problem, the partitions $A$ and $B$ are of equal size; therefore,
\eqref{sumlinkage} is minimized whenever \eqref{avglinkage} is
minimized. Hence, the cluster editing objective is a generalization of
the average linkage measure on more than two clusters: maximizing the
distance criterion maximizes cluster similarity as measured by the
average linkage method.

# Algorithmic approaches

Finding optimal data partitions usually corresponds to NP-complete
problems [@arabie1996; @jain2010; @bansal2004]. For NP-complete
problems, it is often infeasible to find the optimal objective,
especially when *n* is large. To find the optimal solution for
moderately large instances, `anticlust` employs integer linear
programming. To process larger problem instances, `anticlust` uses
heuristic methods based on repeated random sampling.

## NP-completeness

In the following, I show that distance anticlustering is NP-complete.
First, distance anticlustering is in NP because the distance objective
can be computed in polynomial time for a given partitioning; the
summation of all distance values $d_{ij}$ is in $O(n^2)$.

Second, I show that if an efficient algorithm exists to solve distance
anticlustering in polynomial time, it is also possible solve the
NP-complete balanced number partitioning problem in polynomial time
[@mertens2001]. In the number partitioning problem, we have a list of
positive integers $a_1, a_2, ..., a_n$ and try to find a subset $A
\subset \{1, ..., n\}$ that minimizes the partition difference

\begin{align}
E(A) = \bigg\vert \sum\limits_{i \in A} a_i - \sum\limits_{j \notin A} a_j \bigg\vert
\end{align}

In the balanced version of number partitioning, we may impose the
restriction of $\vert A \vert = \frac{n}{2}$ -- assuming that $n$ is
even -- corresponding to restriction \eqref{formalization:3} of equal
cluster sizes in anticlustering [@mertens2001].

To convert the number partitioning formulation into a formulation of
distance anticlustering, we define $d_{ij}$ as the absolute difference
representing the dissimilarity of two numbers:

\begin{align} \label{absdiff}
d_{ij} := \vert a_i - a_j \vert
\end{align}

We thus obtain

\begin{align}
E(A) = \sum\limits_{i \in A} \sum\limits_{j \notin A} d_{ij}
\end{align}

Using variables $x_{ij} \in \{0, 1\}$ to represent whether two numbers
are both either in $A$ or not, i.e.,

$$
   x_{ij} =
   \begin{cases}
     1 & \textrm{if} \:  \: (x_i \in A \wedge x_j \in A) \vee
       (x_i \notin A \wedge x_j \notin A) \\
     0 & \textrm{otherwise}
   \end{cases}
$$

we obtain $E(A)$ as the distance anticlustering objective:

\begin{align}
E(A) = \sum\limits_{1 \leq i < j \leq n} d_{ij} \: x_{ij}
\end{align}

Hence, balanced number partitioning is a special case of distance
anticlustering where

a) $K = 2$
b) each element is described by exactly one integer
c) $d_{ij}$ is the absolute difference

If a polynomial-time algorithm exists that solves distance
anticlustering, we can therefore solve the NP-complete balanced number
partitioning in polynomial time. Hence, distance anticlustering is
NP-complete.

## Integer linear programming

Despite the NP-complete nature of cluster editing, integer linear
programming has been successfully used to find optimal solutions even
for relatively large problem instances [@bocker2011; @lorena2018].
Integer linear programming identifies the values of *decision variables*
that optimize a linear objective function while employing constraints on
the decision variables that are implemented as mathematical
inequalities. In the case of anticlustering, integer linear programming
can be used to maximize the distance objective in \eqref{objectivedist}
while inequalities ensure that the anticlustering conditions in
\eqref{formalization:1} - \eqref{formalization:3} are met.

For the integer linear programming formulation of distance
anticlustering, a problem instance is represented as an undirected
complete graph $G = (V, E)$ [cf. @schaeffer2007]. Each vertex $v \in V
(1, ..., n)$ represents an element from the input data that has to be
assigned to an anticluster. Edges are unordered pairs $\{i, j\} \in E$;
the short form $ij$ will be used to refer to edges. A cost function $w:
E \to \mathbb{R}$ assigns a weight to each edge. In distance
anticlustering, an edge weight is given by the distance between the two
elements that are connected by the edge, i.e., $w(ij) = d_{ij}$.

The integer linear program returns a subgraph $G' = (V, E')$. The
decision variables $x_{ij}$ encode wether two vertices $i$ and $j$ are
connected by an edge in the output graph $G'$:

$$
   x_{ij} =
   \begin{cases}
     1 & \textrm{if} \:  \: ij \in E' \\
     0 & \textrm{otherwise}
   \end{cases}
$$

If two vertices are connected by an edge in $G'$, the integer linear
program ensures that they are also part of a *clique*, that is, a
subgraph whose vertices are all connected. Cliques in $G'$ represent
anticlusters -- hence, the following two conditions are equivalent in
the integer linear programming formulation of distance anticlustering:

1. $ij \in E'$
2. Vertices $i$ and $j$ belong to the same anticluster.

The `anticlust` package employs an integer linear program on the basis
of formulations proposed by GrÃ¶tschel and Wakabayashi [-@grotschel1989]
to solve cluster editing:

\begin{align}
\mathrm{Maximize} \sum\limits_{1 \leq i < j \leq n} w(ij) \: x_{ij} \label{maxobjective} \\
- x_{ij} + x_{ik} + x_{jk} \le 1, \qquad
  \forall \; 1 \le i < j < k \le n, \label{tri1} \\
x_{ij} - x_{ik} + x_{jk} \le 1, \qquad
  \forall \;  1 \le i < j < k \le n, \\
x_{ij} + x_{ik} - x_{jk} \le 1, \qquad
  \forall \; 1 \le i < j < k \le n, \label{tri3} \\
\sum\limits_{1 \leq i < j \leq n} x_{ij} + \sum\limits_{1 \leq k < i \leq n} x_{ki} = \frac{n}{K} - 1, \qquad \forall i \in \{1, ..., n\} \label{clustersize} \\
x_{ij} \in \{0, 1\}, \; \forall \; 1 \leq i < j \leq n \label{binary}
\end{align}

The inequalities \eqref{tri1} - \eqref{tri3} are called triangular
constraints; they ensure that the output graph is a union of disjoint
cliques [@grotschel1989]. Constraint \eqref{clustersize} ensures that
$K$ cliques are returned, each of cardinality $\frac{n}{K}$. Constraint
\eqref{binary} ensures that the decision variables $x_{ij}$ are binary.
Cliques are created under the objective to maximize the sum of the edge
weights within cliques.

In the `anticlust` package, one of the commercial solvers `gurobi` or
`CPLEX`, or the open source GNU linear programming kit can be used to
solve the ILP in \eqref{maxobjective} - \eqref{binary} optimally.

### Preprocessing

To expand its applicability to larger problem sizes, additional
constraints were added to the integer linear program through a
redefinition of the cost function $w$. The redefinition resulted from
two considerations:

1. The run time of the weighted cluster editing ILP is improved if
   there is an uneven distribution of edge weights [@bocker2013;
   @bocker2011]
2. It is possible to prevent very similar elements from joining the same
   anticluster without impairing the quality of the solution strongly

In the cluster editing integer linear programming framework, setting
$w(ij) = -\infty$ prevents two vertices $i$ and $j$ from joining the
same clique in the output graph [@bocker2011].  This recalculation of
some edge weights induces a strong enevenness  among edge weights,
increasing the problem sizes the integer linear program can be applied
to. If only very similar elements are prevented from joining the same
anticluster, the quality of the solution is not impaired by much because
the anticlustering objective is based on the idea that similar elements
should be in different anticlusters.

Therefore, the `anticlust` package realizes a preprocessing step in
which very similar elements are grouped into *preclusters*; elements of
a precluster are prevented from joining the same anticluster thereafter.
The preclustering step is formalized by the ILP defined in
\eqref{clusterediting} - \eqref{clusterediting:end}.

\begin{align}
\mathrm{Minimize} \sum\limits_{1 \leq i < j \leq n} w(ij) \: x_{ij} \label{clusterediting} \\
- x_{ij} + x_{ik} + x_{jk} \le 1, \qquad
  \forall \; 1 \le i < j < k \le n, \\
x_{ij} - x_{ik} + x_{jk} \le 1, \qquad
  \forall \;  1 \le i < j < k \le n, \\
x_{ij} + x_{ik} - x_{jk} \le 1, \qquad
  \forall \; 1 \le i < j < k \le n,  \\
\sum\limits_{1 \leq i < j \leq n} x_{ij} + \sum\limits_{1 \leq k < i \leq n} x_{ki} = K - 1, \qquad \forall i \in \{1, ..., n\} \\
x_{ij} \in \{0, 1\}, \; \forall \; 1 \leq i < j \leq n \label{clusterediting:end}
\end{align}

```{r preparecomputation, echo = FALSE}
n_elements <- 15
features <- matrix(runif(n_elements * 2), ncol = 2)
n_preclusters <- n_elements / 3
n_anticlusters <- n_elements / n_preclusters

clusters <- clustering(features, n_preclusters, method = "exact")
anticlusters <- anticlustering(features, n_anticlusters,
                               method = "exact", preclustering = TRUE)

```

By minimizing the distance objective, the preclustering step solves
weighted cluster editing under the restriction of a cluster size $K$,
resulting in $\frac{n}{K}$ clusters.[^bettermin] The number of elements
per precluster corresponds to the number of anticlusters. For $K = 2$,
the preclustering corresponds to the minimum weight perfect matching
problem [@gerards1995]. The left-hand plot in Figure 2 illustrates the
preprocessing step for $K = `r n_anticlusters`$ and $n = `r 
n_elements`$.

[^bettermin]: This is a feasible preprocessing step because minimizing
the distance objective works faster than maximization for the same
problem size. That is, cluster editing is solved more efficiently using
ILP than anticlustering.

```{r plotanticlusters, fig.width = 6.5, fig.height = 3.7, echo = FALSE, fig.cap = "The left-hand plot shows the preclusters each consisting of three elements. The right-hand plot shows the optimal anticlustering under the preclustering restrictions. Elements that are part of the same precluster cannot be part of the same anticluster"}

# Plot clusters and anticlusters
par(mfrow = c(1, 2))
pch <- 15:(15+n_preclusters-1)
plot_clusters(features, clusters, pch = pch, connect_clusters = TRUE,
              main = "Preclustering", xlab = "", ylab = "")
plot_clusters(features, anticlusters, pch = 15:(15+n_anticlusters-1),
              main = "Anticlustering", xlab = "", ylab = "")
anticlust:::draw_all_cliques(features[, 1], features[, 2],
                             clusters, lty = 2,
                             cols = rep("darkgrey", n_elements))
```

In a second step, the cost function is redefined to prevent elements
from the same preclustering from joining the same anticluster:

$$
   w(ij) =
   \begin{cases}
     -\infty & \textrm{if}  \:  \: x_{ij} = 1 \\
     d_{ij} & \textrm{if} \:  \: x_{ij} = 0
   \end{cases}
$$

Using the redefined edge weights, we solve distance anticlustering using
the ILP defined in \eqref{maxobjective} - \eqref{binary}. The results of
an anticlustering employing the preclustering restrictions is shown in
the right-hand plot of Figure 2. Because the optimal solution sometimes
sometimes requires to join elements that are part of the same
precluster, the preprocessing sometimes precludes an optimal solution.

## Heuristic anticlustering

To solve larger problem instances that cannot be processed using integer
linear programming, two heuristic methods based on random search are
employed in the `anticlust` package: random sampling and simulated
annealing. Both methods may employ a preclustering step that usually
improves the quality of the output.

### Random sampling

A simple random sampling approach may be used to optimize the variance
or distance objective. That is, across a user-specified number of runs,
each element is randomly first assigned to an anticluster, then the
objective value is computed and in the end, the best assignment is
returned as the output. When a preclustering is employed, the random
assignment is conducted under the restriction that preclustered elements
cannot be part of the same anticluster.

### Simulated annealing

The `anticlust` package also employs a simulated annealing approach to
optimize the variance or distance objective. If no preclustering  is
employed, anticlusters are initialized randomly. The exchange step is
realized by changing the anticluster affiliations of two random elements
-- this step is repeated if two elements  of the same anticluster are
swapped.  

If preclustering is employed, the initialization of cluster assignments
incorporates the condition that elements from the same precluster are
not part of the same anticluster. The exchange step is realized as
follows: a precluster is selected at random. From this precluster,  two
elements exchange their anticluster affiliation.

The simulated annealing uses the function `optim` from base `R`; the
following parameters are used:

- Temperature: 2000
- tmax: 20

### Heuristic preclustering

A preclustering step may be used to improve the quality of the random
search procedures. To this end, a heuristic clustering algorithm is
employed. The algorithm is based on k-means clustering and enforces
equal cluster sizes. First, $\frac{n}{K}$ cluster centers are initialed
using k-means; then, elements are sequentially assigned to the nearest
cluster centers while ensuring that each of the clusters is filled with
$K$ elements. The following pseudo code formalizes the heuristic:

```
E = list of n elements
C = list of K/n cluster centers, initialized using k-means
// Iterate over the number of elements per cluster
Repeat K times {
  // Iterate over clusters in random order
  S <- random sequence of C
  for (j in S) {
    1. e = element that is closest to cluster center j
    2. Add e to cluster of j
    3. Remove e from E
  }
}
```

\clearpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

\noindent
