---
title: "Getting started with the anticlust package"
author: "Martin Papenberg"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: gfm
  #html_document
bibliography: lit.bib
---

# anticlust

`anticlust` is an `R` package for »anticlustering«, a method to assign
elements to sets in such a way that the sets are as similar as possible
[@spath1986; @valev1998]. The package `anticlust` was originally
developed to assign items to experimental conditions in experimental
psychology, but it can be applied whenever a user requires that a given
set of elements has to be partitioned into similar subsets. The 
`anticlust` package offers the possibility to create sets that are of
equal size (which is the standard case), but it is also possible to create sets 
of different size, or to only assign a subset of all elements to a set.
The package is still under active developement; expect
changes and improvements before it will be submitted to CRAN. Check out
the [NEWS file](https://github.com/m-Py/anticlust/blob/master/inst/NEWS.md) 
for recent changes.

```{r setup, include = FALSE}
library("anticlust")

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.cap = "",
  message = FALSE,
  warning = FALSE
)

set.seed(123)

```

## Installation

```R
library("devtools") # if not available: install.packages("devtools")
install_github("m-Py/anticlust")
```

## Example

In this initial example, I use the main function `anticlustering()` to
create three similar sets of plants using the classical iris data set:

```{r}
# load the package via
library("anticlust")

anticlusters <- anticlustering(
  iris[, -5],
  K = 3,
  objective = "variance",
  method = "exchange"
)

## The output is a vector that assigns a group (i.e, a number 
## between 1 and K) to each input element:
anticlusters

## Each group has the same number of items:
table(anticlusters)

## Compare the feature means by anticluster:
by(iris[, -5], anticlusters, function(x) round(colMeans(x), 2))
```

## How do I learn about anticlustering

This page contains a quick start on how to employ anticlustering using
the `anticlust` package. So, you should start by simply continuing to
read. More information is available via the following sources:

1. The R help. The main function of the package is `anticlustering()`
and the help page of the function (`?anticlustering`) is useful to learn
more about anticlustering. It provides explanations of all function
parameters and how they relate to the theoretical background of
anticlustering.

2. I created a repository on the [Open Science
Framework](https://osf.io/cd5sr/) that includes materials for a better
understanding of the anticlustering method. For example, I posted the
slides of talks that I gave on the `anticlust` package.

3. There is a paper in preparation that will explain the theoretical
background of the `anticlust` package in detail.

## A quick start

As illustrated in the example above, we can use the function
`anticlustering()` to create similar sets of elements. The function
takes as input a data table describing the elements that should be
assigned to sets. In the data table, each row represents an element, for
example a person, word or a photo. Each column is a numeric variable
describing one of the elements' features. The table may be an R `matrix`
or `data.frame`; a single feature can also be passed as a `vector`. The
number of groups is specified through the argument `K`.

### The anticlustering objective

To quantify set similarity, `anticlust` may employ one of two measures
that have been developed in the context of cluster analysis:

- the k-means "variance" objective [@spath1986; @valev1998] 
- the cluster editing "distance" objective [@bocker2013; @miyauchi2015; @grotschel1989]

The k-means objective is given by the sum of the squared distances
between cluster centers and individual data points [@jain2010]. The
cluster editing objective is the sum of pairwise distances within each
anticluster.  The following plot illustrates both objectives for 15 
elements that have been assigned to three sets. Each element is 
described by two numeric features, displayed as the x- and y-axis:

```{r, out.width = '100%', echo = FALSE}
knitr::include_graphics("objectives_updated.png")
```


```{r, echo = FALSE, eval = FALSE}

## Create N random elements:
N <- 12
features <- matrix(rnorm(N * 2), ncol = 2)
K <- 3

## Generate all possible partitions to divide N items in K sets:
partitions <- generate_partitions(K, N)

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
var_obj <- function(clusters, features) {
  variance_objective(features, clusters)
}

dist_obj <- function(clusters, features) {
  distance_objective(features, clusters = clusters)
}

var_objectives <- sapply(
  partitions,
  FUN = var_obj,
  features = features
)

dist_objectives <- sapply(
  partitions,
  FUN = dist_obj,
  features = features
)

## Select the best partitions:
max_var <- partitions[var_objectives == max(var_objectives)][[1]]
min_var <- partitions[var_objectives == min(var_objectives)][[1]]
max_dist <- partitions[dist_objectives == max(dist_objectives)][[1]]
min_dist <- partitions[dist_objectives == min(dist_objectives)][[1]]

cols <- c("#ff0000", "#1f5a07", "#ABCDEF")

## Plot minimum and maximum objectives:
par(mfrow = c(2, 2))
plot_clusters(
  features,
  max_var,
  illustrate_variance = TRUE,
  main = "Maximum variance",
  col = cols,
  pch = 3:5
)
plot_clusters(
  features,
  max_dist,
  within_connection = TRUE,
  main = "Maximum distance",
  col = cols,
  pch = 3:5
)
plot_clusters(
  features,
  min_var,
  illustrate_variance = TRUE,
  main = "Minimum variance",
  col = cols,
  pch = 3:5
)
plot_clusters(
  features,
  min_dist,
  within_connection = TRUE,
  main = "Minimum distance",
  col = cols,
  pch = 3:5
)

```

The lines connecting the dots illustrate the distances that enter the
objective functions. For anticluster editing ("distance objective"),
lines are drawn between pairs of elements within the same anticluster,
because the objective is the sum of the pairwise distances between
elements in the same cluster.  For k-means anticlustering ("variance
objective"), lines are drawn between each element and the cluster
centroid, because the objective is the sum of the squared distances
between cluster centers and elements.

Minimizing either the distance or the variance objective creates three
distinct clusters of elements (as shown in the upper plots), whereas
maximization leads to a strong overlap of the three sets, i.e., three
anticlusters (as shown in the lower plots). For anticlustering, the
distance objective maximizes the average similarity between elements in
different sets, whereas the variance objective tends to maximize the
similarity of the cluster centers (i.e., the feature means).

To vary the objective function in the `anticlust` package, we can change
the parameter `objective`. To use anticluster editing, use
`objective = "distance"` (this is also the default). To maximize the
k-means variance objective, set `objective = "variance"`.

```{r, eval = FALSE}

## Example code for varying the objective:
anticlustering(
  features, 
  K = 3, 
  objective = "distance"
)

anticlustering(
  features, 
  K = 3, 
  objective = "variance"
)
```


### Exact anticluster editing

Finding an optimal assignment of elements to sets that maximizes the
anticluster editing or variance objective is computationally demanding.
For anticluster editing, the package `anticlust` still offers the
possibility to find the best possible assignment, relying on [integer
linear programming](https://en.wikipedia.org/wiki/Integer_programming).
This exact approach employs a formulation developed by @grotschel1989,
which has been used to rather efficiently solve the cluster editing
problem [@bocker2011]. To obtain an optimal solution, a linear
programming solver must be installed on your system; `anticlust`
supports the commercial solvers [gurobi](https://www.gurobi.com/) and
[CPLEX](https://www.ibm.com/analytics/cplex-optimizer) as well as the
open source [GNU linear programming
kit](https://www.gnu.org/software/glpk/glpk.html). The commercial
solvers are generally faster. Researchers can install a commercial
solver for free using an academic licence. To use any of the solvers
from within `R`, one of the interface packages `gurobi` (is shipped with
the software gurobi),
[Rcplex](https://CRAN.R-project.org/package=Rcplex) or
[Rglpk](https://CRAN.R-project.org/package=Rglpk) must also be
installed.

To find the optimal solution, we have to set the argument
`method = "ilp"`:

```{r, eval = FALSE}
## Code example for using integer linear programming:
anticlustering(
  features, 
  K = 2, 
  method = "ilp",
  objective = "distance" # "variance" does not work with ILP method
)
```

The "variance" objective cannot be optimized to optimality using integer
linear programming. Check out the help page for the `anticlust` function
`generate_partitions` (`?generate_partitions`) to see how k-means
anticlustering can nevertheless be solved optimally using complete
enumeration (only feasible for small N).

### Preclustering

The exact integer linear programming approach will only work for
moderate problem sizes. We can increase the problem size that can be
handled by setting the argument `preclustering = TRUE`. In this case, an
initial cluster editing is performed, creating small groups of elements
that are very similar. The preclustering step identifies pairs of
similar stimuli if K = 2, triplets if K = 3, and so forth. After this
preclustering, a restriction is enforced to the integer linear program
that precludes very similar elements to be assigned to the same set.

The preclustering restrictions improve the running time of the integer
linear programming solver by a large margin (often 100x as fast) because
many possible assignment are rendered illegal; the integer linear
programming solver is smart and disregards these assignments from the
space of feasible solutions In some occasions, these restrictions
prohibit the integer linear programming solver to find the best solution
because this may be only obtained when some of the preclustered elements
are assigned to the same anticluster. However, in general, the solution
is still very good and often optimal. This code can be used to employ
integer linear programming under preclustering constraints.

```{r, eval = FALSE}
## Code example using ILP and preclustering:
anticlustering(
  features, 
  K = 2, 
  method = "ilp", 
  objective = "distance",
  preclustering = TRUE
)
```

### Heuristic anticlustering

In addition to the exact approach---that is only feasible for small
N---the `anticlust` package may employ two heuristic approaches. The
first option is repeated random sampling: Across a specified number of
runs, anticlusters are assigned randomly. In the end, the assignment
that maximized set similarity is returned. The second approach is an
exchange method: Building on an initial random assignment of elements to
clusters, items are swapped between clusters in such a way that each
swap improves set similarity by the largest amount that is possible
(cf. Späth, 1986).The exchange method is generally prefered because it
usually results in more similar sets and it is also the default method
for the `anticlustering()` function. The following code illustrates how
to vary between the heuristic methods:

```{r, eval = FALSE}
## Code example using random sampling
anticlustering(
  features, 
  K = 2, 
  method = "sampling"
)

## Code example using the exchange method
anticlustering(
  features, 
  K = 2, 
  method = "exchange"
)
```

### Categorical constraints

Sometimes, it is required that sets are not only similar with regard to
some numeric variables, but we also want to ensure that each set
contains an equal number of elements of a certain category. Coming back
to the initial iris data set, we may want to require that each set has a
balanced number of plants of the three iris species. To this end, we can
use the argument `categories` as follows:

```{r}
anticlusters <- anticlustering(
  iris[, -5],
  K = 3,
  objective = "variance",
  method = "exchange",
  categories = iris[, 5]
)

## The species are as balanced as possible across anticlusters:
table(anticlusters, iris[, 5])

```

## Questions and suggestions

If you have any question on the `anticlust` package or any suggestions
(which are greatly appreciated), I encourage you to contact me via email
(martin.papenberg@hhu.de) or [Twitter](https://twitter.com/MPapenberg),
or to open an issue on this Github repository.

## References
